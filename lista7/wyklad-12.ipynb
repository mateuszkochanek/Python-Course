{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykład 12: Projektowanie sieci ConvNet\n",
    "\n",
    "Na tym wykładzie zaprojektujemy sami sieci ConvNet i prostymi zastosowaniami\n",
    "\n",
    "Zobaczymy także trzy różne metody definiowania modeli w Kerasie:\n",
    "\n",
    "- Sekewncyjny (Sequential)\n",
    "- Funcjonalny (Functional)\n",
    "- Zorientowany obietkowo (Object-Oriented)\n",
    "\n",
    "Uwaga: Celem tego wykładu nie jest porównanie wydajności, ale tylko samych architektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytywanie danych (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((50000, 28, 28, 1), (50000,)),\n",
       " ((10000, 28, 28, 1), (10000,)),\n",
       " ((10000, 28, 28, 1), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
    "h, w = x_train.shape[1:]\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], h, w, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], h, w, 1)\n",
    "input_shape = (h, w, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=10000, random_state=42)\n",
    "\n",
    "(x_train.shape, y_train.shape), (x_val.shape, y_val.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(x_train[0].squeeze(-1))\n",
    "plt.title(y_train[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] unique labels.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"{} unique labels.\".format(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LeNet\n",
    "\n",
    "Na początku zdefiniujemy klasyczny model LeNet-5 wprowadzony przez Yann Le Cun in 1998 ([url](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)). Model jest na tyle prosty, że możemy go łatwo zdefiniować z wykorzystaniem **Sequential** API.\n",
    "\n",
    "![lenet archi](images/lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LeNet-5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "C1 (Conv2D)                  (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "S2 (MaxPooling2D)            (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "C3 (Conv2D)                  (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "S4 (AveragePooling2D)        (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "C5 (Conv2D)                  (None, 1, 1, 120)         48120     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "F6 (Dense)                   (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AvgPool2D, MaxPool2D, Dense, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "lenet = Sequential(name=\"LeNet-5\")\n",
    "\n",
    "lenet.add(Conv2D(6, kernel_size=(5, 5), activation=\"tanh\", padding=\"same\", input_shape=input_shape, name=\"C1\"))\n",
    "lenet.add(MaxPool2D(pool_size=(2, 2), name=\"S2\"))\n",
    "lenet.add(Conv2D(16, kernel_size=(5, 5), activation='tanh', name=\"C3\"))\n",
    "lenet.add(AvgPool2D(pool_size=(2, 2), name=\"S4\"))\n",
    "lenet.add(Conv2D(120, kernel_size=(5, 5), activation='tanh', name=\"C5\"))\n",
    "lenet.add(Flatten())\n",
    "lenet.add(Dense(84, activation='tanh', name=\"F6\"))\n",
    "lenet.add(Dense(10, activation='softmax'))\n",
    "\n",
    "lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 11s 54ms/step - loss: 0.6170 - accuracy: 0.8386 - val_loss: 0.3270 - val_accuracy: 0.9004\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 11s 54ms/step - loss: 0.2466 - accuracy: 0.9275 - val_loss: 0.2115 - val_accuracy: 0.9350\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 10s 53ms/step - loss: 0.1711 - accuracy: 0.9504 - val_loss: 0.1439 - val_accuracy: 0.9577\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 11s 57ms/step - loss: 0.1300 - accuracy: 0.9616 - val_loss: 0.1118 - val_accuracy: 0.9693\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 12s 63ms/step - loss: 0.1059 - accuracy: 0.9688 - val_loss: 0.0963 - val_accuracy: 0.9731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6be82061d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "lenet.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "lenet.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0900 - accuracy: 0.9723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09002140164375305, 0.9722999930381775]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga: LeNet został na początku zdefiniowany z wykorzystaniem funkcji aktywacji `tanh` lub `sigmoid`, aktualnie\n",
    "te funkcje aktywacji są rzadko używane. Można pokazać, że obie funkcje \"nasycają się\" (saturate)\n",
    "dla bardzo małych i bardzo dużych wartości co powoduje \"zanik\" gradientu.\n",
    "\n",
    "Dlatego aktualnie większość sieci wykorzystuje `ReLU` w warstwach ukrytych lub inne o podobnych własnościach\n",
    "(https://keras.io/layers/advanced-activations/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inception\n",
    "\n",
    "Model \"Inception\" został wprowadzony w 2014 przez Szegedy et al. ([paper url](https://arxiv.org/abs/1409.4842)).\n",
    "\n",
    "Konwolucje mają efektywne \"pole chłonności\" (receptive field): im większe \"kernels\" i im głębszy (deeper) model, tym więcej pikseli obrazu *zobaczymy*. Dobrze wyjaśnienia to: [medium blog](https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807).\n",
    "\n",
    "W modelu tym, różne \"convolution kernels\" są ze sobą łączone. Mniejsze \"kernels\" wykorzystywane są do mniejszych\n",
    "klastrów cech (można myśleć o tym jak o detalach np. oko), podchas duże \"kernels\" dla większych kastrów cech\n",
    "(np. twarz)\n",
    "\n",
    "![inception archi](images/inception.png)\n",
    "\n",
    "Tym razem wykorzystamy **Functional** API  do zdefiniowania pojedynczej warstwy \"Inception layer\"\n",
    "\n",
    "Przykład:\n",
    "\n",
    "```python\n",
    "a = Input(shape=(32,))\n",
    "b = Dense(32)(a)\n",
    "model = Model(inputs=a, outputs=b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 28, 28, 16)   416         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 28, 28, 32)   544         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 28, 28, 32)   12832       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 28, 28, 32)   4640        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 28, 28, 16)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 28, 28, 112)  0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "                                                                 max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 87808)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           878090      flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 896,522\n",
      "Trainable params: 896,522\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def inception_layer(tensor, n_filters):\n",
    "    branch1x1 = Conv2D(n_filters, kernel_size=(1, 1), activation=\"relu\", padding=\"same\")(tensor)\n",
    "    branch5x5 = Conv2D(n_filters, kernel_size=(5, 5), activation=\"relu\", padding=\"same\")(tensor)\n",
    "    branch3x3 = Conv2D(n_filters, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")(tensor)\n",
    "\n",
    "    branch_pool = MaxPool2D(pool_size=(3, 3), strides=(1, 1), padding=\"same\")(tensor)\n",
    "\n",
    "    output = Concatenate(axis=-1)(\n",
    "        [branch1x1, branch5x5, branch3x3, branch_pool]\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=input_shape)\n",
    "x = Conv2D(16, kernel_size=(5, 5), padding=\"same\")(input_tensor)\n",
    "x = inception_layer(x, 32)\n",
    "x = Flatten()(x)\n",
    "output_tensor = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "mini_inception = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "mini_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "196/196 [==============================] - 90s 461ms/step - loss: 0.3232 - accuracy: 0.9032 - val_loss: 0.1722 - val_accuracy: 0.9467\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 83s 423ms/step - loss: 0.1115 - accuracy: 0.9687 - val_loss: 0.1655 - val_accuracy: 0.9479\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 83s 424ms/step - loss: 0.0717 - accuracy: 0.9792 - val_loss: 0.0739 - val_accuracy: 0.9773\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 83s 424ms/step - loss: 0.0578 - accuracy: 0.9818 - val_loss: 0.0704 - val_accuracy: 0.9798\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 83s 426ms/step - loss: 0.0485 - accuracy: 0.9852 - val_loss: 0.0590 - val_accuracy: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6ba47230b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "mini_inception.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "mini_inception.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 15ms/step - loss: 0.0467 - accuracy: 0.9843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04670601338148117, 0.9843000173568726]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_inception.evaluate(x_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet\n",
    "\n",
    "ResNet (*Residual Networks*) model został wprowadzony przez He et al. in 2015 ([paper url](https://arxiv.org/abs/1512.03385)). Autorzy pracy zauważyli, że więcej warstw znacznie poprawia wydajność sieci, niestety\n",
    "bardzo trudno uczyć takie sieci (backpropagate the gradients).\n",
    "\n",
    "Trik który pozwalał uczyć takie sieci umożliwiając łatwiejszy \"przepływ\" gradientu polega na wprowadzeniu skrótów\n",
    "\n",
    "![resnet archi](images/resnet.png)\n",
    "\n",
    "Tym razem wykorzystamy **Oriented-Object** API:\n",
    "\n",
    "Przykład:\n",
    "```python\n",
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.classifier(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mini_res_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            multiple                  832       \n",
      "_________________________________________________________________\n",
      "ResidualBlock (ResidualBlock multiple                  18496     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  250890    \n",
      "=================================================================\n",
      "Total params: 270,218\n",
      "Trainable params: 270,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Add, Layer, Activation\n",
    "\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__(name=\"ResidualBlock\")\n",
    "\n",
    "        self.conv1 = Conv2D(n_filters, kernel_size=(3, 3), activation=\"relu\", padding=\"same\")\n",
    "        self.conv2 = Conv2D(n_filters, kernel_size=(3, 3), padding=\"same\")\n",
    "        self.add = Add()\n",
    "        self.last_relu = Activation(\"relu\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(inputs)\n",
    "\n",
    "        y = self.add([x, inputs])\n",
    "        y = self.last_relu(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class MiniResNet(Model):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = Conv2D(n_filters, kernel_size=(5, 5), padding=\"same\")\n",
    "        self.block = ResidualBlock(n_filters)\n",
    "        self.flatten = Flatten()\n",
    "        self.classifier = Dense(10, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.block(x)\n",
    "        x = self.flatten(x)\n",
    "        y = self.classifier(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "mini_resnet = MiniResNet(32)\n",
    "mini_resnet.build((None, *input_shape))\n",
    "mini_resnet.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ResidualBlock/conv2d_5/kernel:0', 'ResidualBlock/conv2d_5/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ResidualBlock/conv2d_5/kernel:0', 'ResidualBlock/conv2d_5/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ResidualBlock/conv2d_5/kernel:0', 'ResidualBlock/conv2d_5/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['ResidualBlock/conv2d_5/kernel:0', 'ResidualBlock/conv2d_5/bias:0'] when minimizing the loss.\n",
      "196/196 [==============================] - 40s 205ms/step - loss: 0.3430 - accuracy: 0.9022 - val_loss: 0.1386 - val_accuracy: 0.9593\n",
      "Epoch 2/5\n",
      "196/196 [==============================] - 39s 200ms/step - loss: 0.1127 - accuracy: 0.9668 - val_loss: 0.1022 - val_accuracy: 0.9687\n",
      "Epoch 3/5\n",
      "196/196 [==============================] - 39s 199ms/step - loss: 0.0848 - accuracy: 0.9742 - val_loss: 0.0996 - val_accuracy: 0.9697\n",
      "Epoch 4/5\n",
      "196/196 [==============================] - 39s 199ms/step - loss: 0.0698 - accuracy: 0.9794 - val_loss: 0.0746 - val_accuracy: 0.9780\n",
      "Epoch 5/5\n",
      "196/196 [==============================] - 39s 199ms/step - loss: 0.0602 - accuracy: 0.9811 - val_loss: 0.0786 - val_accuracy: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6bec02e6d8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 256\n",
    "\n",
    "mini_resnet.compile(\n",
    "    optimizer=optimizers.SGD(lr=0.1),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "mini_resnet.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0677 - accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06772437691688538, 0.9783999919891357]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_resnet.evaluate(x_test, y_test, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
